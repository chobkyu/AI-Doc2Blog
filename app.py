# import zipfile
# import os
# from docx import Document
# import json

# ZIP_PATH = 'ì›ì§ PT í‰ì´Œë³¸ì .zip'  # â† ë„¤ zip íŒŒì¼ ì´ë¦„ìœ¼ë¡œ ë³€ê²½
# EXTRACT_DIR = 'unzipped'

# # 1. zip ì••ì¶• í’€ê¸°
# with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:
#     zip_ref.extractall(EXTRACT_DIR)

# # 2. .docx ì°¾ê¸°
# docx_files = [f for f in os.listdir(EXTRACT_DIR) if f.endswith('.docx')]
# if not docx_files:
#     raise Exception("ZIP íŒŒì¼ ì•ˆì— .docx íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

# docx_path = os.path.join(EXTRACT_DIR, docx_files[0])
# doc = Document(docx_path)


# # 3. í‘œ ì¶”ì¶œ í•¨ìˆ˜
# def extract_tables(doc):
#     tables = []
#     for table in doc.tables:
#         rows = []
#         for row in table.rows:
#             cells = [cell.text.strip() for cell in row.cells]
#             rows.append(cells)
#         tables.append(rows)

#     parsed_data = {}
#     for item in tables[0]:
#         if len(item) == 2:
#             key = item[0].replace('\n', ' ').strip() # ì¤„ë°”ê¿ˆ ë¬¸ìë¥¼ ê³µë°±ìœ¼ë¡œ ë°”ê¾¸ê³  ì•ë’¤ ê³µë°± ì œê±°
#             value = item[1].strip() # ê°’ì˜ ì•ë’¤ ê³µë°± ì œê±°

#             # 'â€» í¬ìŠ¤íŒ… ê°€ì´ë“œ'ì²˜ëŸ¼ ì •ë³´ê°€ ì—†ëŠ” ì¤‘ë³µ í‚¤ëŠ” ê±´ë„ˆëœ€
#             if key == 'â€» í¬ìŠ¤íŒ… ê°€ì´ë“œ' and value == 'â€» í¬ìŠ¤íŒ… ê°€ì´ë“œ':
#                 continue
            
#             parsed_data[key] = value
    
#     print(parsed_data)
#     return parsed_data

# # def convert_to_json(table):
# #     headers = table[0]
# #     return [dict(zip(headers, row)) for row in table[1:]]

# # # 4. ì‹¤í–‰
# tables = extract_tables(doc)
# # for idx, table in enumerate(tables):
# #     print(f'\nğŸ“„ í…Œì´ë¸” {idx + 1}')
# #     print(json.dumps(convert_to_json(table), ensure_ascii=False, indent=2))


# import openai
# import os

# # í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ë¡œë“œ
# openai.api_key = ""

# if openai.api_key is None:
#     print("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
# else:
    
#     prompt = f"""
# You are an AI assistant specialized in generating engaging and SEO-optimized Naver blog posts. Your task is to write a blog review based on the provided input data.

# Here are the requirements for the blog post:

# 1.  **Audience & Tone:** Write a friendly and engaging review, suitable for an "after-use" experience.
# 2.  **Mobile Optimization:** Ensure the content is **extremely optimized for mobile viewing, prioritizing maximum readability on small screens.**
#     * **Line Length:** Every line must be very short. **Even if a sentence is long, break it into multiple lines using a newline character (`\n`) after every few words or a natural phrase break.** Aim for lines that are typically no more than 10-15 characters wide (Korean characters), ensuring a very punchy, easy-to-scan format.
#     * **Paragraph Separation:** **After each main paragraph, you MUST insert two blank lines (i.e., three newline characters: `\n\n\n`) to create a very distinct visual separation.** This ensures that when printed, there are clearly two empty lines between paragraphs.
# 3.  **Introduction:** Start the blog post with a welcoming greeting.
# 4.  **SEO & Structure:**
#     * Write the post with Naver blog SEO best practices in mind.
#     * Divide the main content into 4-5 distinct paragraphs.
#     * **Paragraph Content Guidance:** Each main paragraph should focus on a specific aspect of the experience, similar to these examples:
#         * Paragraph 1: Initial impressions (e.g., interior, facilities, cleanliness, modern feel).
#         * Paragraph 2: Instructor's quality (e.g., friendliness, personalized program design, attention to detail, expectation of results).
#         * Paragraph 3-5: Continue with other positive aspects derived from the input data, such as program effectiveness, atmosphere, or specific benefits.
# 5.  **Word Count:** The total word count must be at least 450 characters (Korean characters).
# 6.  **Keywords & Company Name:** Incorporate the provided keyword "{tables['í‚¤ì›Œë“œ']}" and company name "{tables['ìƒí˜¸ (ìƒí’ˆ/ì„œë¹„ìŠ¤)']}" naturally throughout the text.
# 7.  **Title Suggestion:** Suggest one title following this format: `[~ëŠ” + keyword + company name]`.
# 8.  **Hashtag Recommendation:** Recommend at least 10 relevant hashtags.
# 9.  **Closing:** Conclude the post by including the phrase "ê°ì‚¬í•©ë‹ˆë‹¤" (Thank you).



#         **Input Data (Parsed from document):**
#         {str(tables['í¬ìŠ¤íŒ… ì°¸ê³  ë‚´ìš©'])}

#         **Expected Output Language:** Korean
#     """
#     completion = openai.chat.completions.create(
#         model="gpt-4.1-nano",  # ë˜ëŠ” "gpt-4", "gpt-4o" ë“±
#         messages=[
#             {"role": "system", "content": "You are a helpful assistant."},
#             {"role": "user", "content": prompt}
#         ],
#         max_tokens=1000,  # ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜
#         temperature=0.7, # ì°½ì˜ì„± ì¡°ì ˆ (0.0: ë³´ìˆ˜ì , 1.0: ì°½ì˜ì )
#     )

#     print(completion.choices[0].message.content)

import zipfile
import os
from docx import Document
import openai
import json
from dotenv import load_dotenv

load_dotenv()

ZIP_FOLDER = 'documents'
RESULT_FOLDER = 'result'

# OpenAI API í‚¤ ì„¤ì •
openai.api_key = os.getenv("OPENAI_API_KEY")

# result í´ë” ìƒì„±
os.makedirs(RESULT_FOLDER, exist_ok=True)

# 1. ë¬¸ì„œì—ì„œ í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ í•¨ìˆ˜
def extract_tables(doc):
    tables = []
    for table in doc.tables:
        rows = []
        for row in table.rows:
            cells = [cell.text.strip() for cell in row.cells]
            rows.append(cells)
        tables.append(rows)

    parsed_data = {}
    for item in tables[0]:
        if len(item) == 2:
            key = item[0].replace('\n', ' ').strip()
            value = item[1].strip()
            if key == 'â€» í¬ìŠ¤íŒ… ê°€ì´ë“œ' and value == 'â€» í¬ìŠ¤íŒ… ê°€ì´ë“œ':
                continue
            parsed_data[key] = value
    return parsed_data

# 2. ê° zip íŒŒì¼ ë°˜ë³µ ì²˜ë¦¬
for zip_filename in os.listdir(ZIP_FOLDER):
    if not zip_filename.endswith('.zip'):
        continue

    zip_path = os.path.join(ZIP_FOLDER, zip_filename)
    zip_name_without_ext = os.path.splitext(zip_filename)[0]

    # ì••ì¶• í•´ì œ ê²½ë¡œ
    extract_path = os.path.join(RESULT_FOLDER, zip_name_without_ext)
    os.makedirs(extract_path, exist_ok=True)

    # zip ì••ì¶• í’€ê¸°
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_path)

    # .docx ì°¾ê¸°
    docx_files = [f for f in os.listdir(extract_path) if f.endswith('.docx')]
    if not docx_files:
        print(f"âŒ {zip_filename} ì•ˆì— .docx íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        continue

    docx_path = os.path.join(extract_path, docx_files[0])
    doc = Document(docx_path)

    tables = extract_tables(doc)

    # OpenAI í”„ë¡¬í”„íŠ¸ ì‘ì„±
    prompt = f"""
You are an AI assistant specialized in generating engaging and SEO-optimized Naver blog posts. Your task is to write a blog review based on the provided input data.

Here are the requirements for the blog post:

1.  **Audience & Tone:** Write a friendly and engaging review, suitable for an "after-use" experience.
2.  **Mobile Optimization:** Ensure the content is **extremely optimized for mobile viewing, prioritizing maximum readability on small screens.**
    * **Line Length:** Every line must be very short. **Even if a sentence is long, break it into multiple lines using a newline character (`\n`) after every few words or a natural phrase break.** Aim for lines that are typically no more than 10-15 characters wide (Korean characters), ensuring a very punchy, easy-to-scan format.
    * **Paragraph Separation:** **After each main paragraph, you MUST insert two blank lines (i.e., three newline characters: `\n\n\n`) to create a very distinct visual separation.**
3.  **Introduction:** Start the blog post with a welcoming greeting.
4.  **SEO & Structure:**
    * Write the post with Naver blog SEO best practices in mind.
    * Divide the main content into 4-5 distinct paragraphs.
    * **Paragraph Content Guidance:** Each main paragraph should focus on a specific aspect of the experience, such as:
        * Interior/facilities/cleanliness
        * Instructor's friendliness and program
        * Effectiveness, results, etc.
5.  **Word Count:** The total word count must be at least 450 characters (Korean characters).
6.  **Keywords & Company Name:** Incorporate the provided keyword "{tables.get('í‚¤ì›Œë“œ', '')}" and company name "{tables.get('ìƒí˜¸ (ìƒí’ˆ/ì„œë¹„ìŠ¤)', '')}" naturally throughout the text.
7.  **Title Suggestion:** Suggest one title following this format: `[~ëŠ” + keyword + company name]`.
8.  **Hashtag Recommendation:** Recommend at least 10 relevant hashtags.
9.  **Closing:** Conclude the post by including the phrase "ê°ì‚¬í•©ë‹ˆë‹¤" (Thank you).


**Input Data (Parsed from document):**
{tables.get('í¬ìŠ¤íŒ… ì°¸ê³  ë‚´ìš©', '')}

**Expected Output Language:** Korean
"""

    try:
        completion = openai.chat.completions.create(
            model="gpt-4.1-nano",  # ë˜ëŠ” "gpt-4", "gpt-4.1-nano"
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=1500,
            temperature=0.7
        )

        result_text = completion.choices[0].message.content

        # ê²°ê³¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
        output_path = os.path.join(extract_path, 'result.txt')
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(result_text)

        print(f"âœ… ì™„ë£Œ: {zip_filename} â†’ {output_path}")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {zip_filename}")
        print(e)
